{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. a) Simple three layer MLP - As per tutorial"
      ],
      "metadata": {
        "id": "JM92M0CfJF5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# The mnist.load_data() method is convenient, as there is no need to load all 70,000\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# The labels are in the form of digits, from 0 to 9.\n",
        "num_labels = len(np.unique(y_train))\n",
        "print(\"total labels:{}\".format(num_labels))\n",
        "print(\"labels:{0}\".format(np.unique(y_train)))\n",
        "\n",
        "# The most suitable format is one-hot, a 10-dimensional vector-like all 0 values, except the class index.\n",
        "#converter em one-hot\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "input_size = image_size * image_size\n",
        "\n",
        "# Our model is an MLP, so inputs must be a 1D tensor. as such, x_train and x_test must be transformed into [60,000, 2828] and [10,000, 2828],\n",
        "print(\"x_train:t{}\".format(x_train.shape))\n",
        "print(\"x_test:tt{}n\".format(x_test.shape))\n",
        "\n",
        "x_train = np.reshape(x_train, [-1, input_size])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "\n",
        "x_test = np.reshape(x_test, [-1, input_size])\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "print(\"x_train:t{}\".format(x_train.shape))\n",
        "print(\"x_test:tt{}\".format(x_test.shape))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "# Parameters\n",
        "batch_size = 128 # It is the sample size of inputs to be processed at each training stage.\n",
        "hidden_units = 256\n",
        "dropout = 0.45\n",
        "\n",
        "# Nossa  MLP com ReLU e Dropout\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Dense(num_labels))\n",
        "\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "_, acc = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "print(\"nAccuracy: %.1f%%n\" % (100.0 * acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4H0pqVeSY0a",
        "outputId": "a438d725-6af0-4018-d968-c3e814f74d47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total de labels:t10\n",
            "labels:ttt[0 1 2 3 4 5 6 7 8 9]\n",
            "x_train:t(60000, 28, 28)\n",
            "x_test:tt(10000, 28, 28)n\n",
            "x_train:t(60000, 784)\n",
            "x_test:tt(10000, 784)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               200960    \n",
            "                                                                 \n",
            " activation (Activation)     (None, 256)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 269322 (1.03 MB)\n",
            "Trainable params: 269322 (1.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "469/469 [==============================] - 6s 10ms/step - loss: 0.4281 - accuracy: 0.8687\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1950 - accuracy: 0.9421\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.1496 - accuracy: 0.9550\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1279 - accuracy: 0.9607\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1131 - accuracy: 0.9656\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1037 - accuracy: 0.9682\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0950 - accuracy: 0.9707\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0857 - accuracy: 0.9730\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0826 - accuracy: 0.9741\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0786 - accuracy: 0.9757\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0707 - accuracy: 0.9769\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0696 - accuracy: 0.9776\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0659 - accuracy: 0.9787\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0650 - accuracy: 0.9793\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0619 - accuracy: 0.9803\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0624 - accuracy: 0.9804\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0571 - accuracy: 0.9817\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0553 - accuracy: 0.9825\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0543 - accuracy: 0.9827\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0536 - accuracy: 0.9823\n",
            "nAccuracy: 98.4%n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.b) An alternate implementation of the MLP without using Keras, but instead using NumPy for building and training the model. It follows a similar architecture as the Keras model as per the tutorial mentioned in the assignment."
      ],
      "metadata": {
        "id": "Fz4MnBK5ZqBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "num_labels = 10\n",
        "input_size = 28 * 28\n",
        "\n",
        "x_train = x_train.reshape(-1, input_size).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, input_size).astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, num_labels)\n",
        "y_test = to_categorical(y_test, num_labels)\n",
        "\n",
        "# Parameters\n",
        "batch_size = 128\n",
        "hidden_units = 256\n",
        "dropout = 0.45\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "\n",
        "# Initialize weights and biases\n",
        "'''\n",
        "weights1 = np.random.randn(input_size, hidden_units)\n",
        "bias1 = np.zeros((1, hidden_units))\n",
        "\n",
        "weights2 = np.random.randn(hidden_units, hidden_units)\n",
        "bias2 = np.zeros((1, hidden_units))\n",
        "\n",
        "weights3 = np.random.randn(hidden_units, num_labels)\n",
        "bias3 = np.zeros((1, num_labels))\n",
        "'''\n",
        "# Activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Softmax function\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Xavier/Glorot initialization\n",
        "def initialize_weights(input_size, output_size):\n",
        "    limit = np.sqrt(6 / (input_size + output_size))\n",
        "    return np.random.uniform(-limit, limit, size=(input_size, output_size))\n",
        "\n",
        "# Initialize weights and biases using Xavier/Glorot initialization\n",
        "weights1 = initialize_weights(input_size, hidden_units)\n",
        "bias1 = np.zeros((1, hidden_units))\n",
        "\n",
        "weights2 = initialize_weights(hidden_units, hidden_units)\n",
        "bias2 = np.zeros((1, hidden_units))\n",
        "\n",
        "weights3 = initialize_weights(hidden_units, num_labels)\n",
        "bias3 = np.zeros((1, num_labels))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle the training data\n",
        "    indices = np.arange(len(x_train))\n",
        "    np.random.shuffle(indices)\n",
        "    x_train = x_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    # Mini-batch training\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch = x_train[i:i + batch_size]\n",
        "        y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        hidden_layer_input = np.dot(x_batch, weights1) + bias1\n",
        "        hidden_layer_output = relu(hidden_layer_input)\n",
        "        hidden_layer_output = hidden_layer_output * (1 - dropout)  # Apply dropout\n",
        "\n",
        "        hidden_layer_input2 = np.dot(hidden_layer_output, weights2) + bias2\n",
        "        hidden_layer_output2 = relu(hidden_layer_input2)\n",
        "        hidden_layer_output2 = hidden_layer_output2 * (1 - dropout)  # Apply dropout\n",
        "\n",
        "        output_layer_input = np.dot(hidden_layer_output2, weights3) + bias3\n",
        "        predicted_output = softmax(output_layer_input)\n",
        "\n",
        "        # Loss calculation (cross-entropy)\n",
        "        loss = -np.sum(y_batch * np.log(predicted_output)) / len(x_batch)\n",
        "\n",
        "        # Backpropagation\n",
        "        output_error = predicted_output - y_batch\n",
        "        hidden_error2 = output_error.dot(weights3.T) * (hidden_layer_output2 > 0)\n",
        "        hidden_error = hidden_error2.dot(weights2.T) * (hidden_layer_output > 0)\n",
        "\n",
        "        # Update weights and biases\n",
        "        weights3 -= learning_rate * hidden_layer_output2.T.dot(output_error) / len(x_batch)\n",
        "        bias3 -= learning_rate * np.sum(output_error, axis=0, keepdims=True) / len(x_batch)\n",
        "\n",
        "        weights2 -= learning_rate * hidden_layer_output.T.dot(hidden_error2) / len(x_batch)\n",
        "        bias2 -= learning_rate * np.sum(hidden_error2, axis=0, keepdims=True) / len(x_batch)\n",
        "\n",
        "        weights1 -= learning_rate * x_batch.T.dot(hidden_error) / len(x_batch)\n",
        "        bias1 -= learning_rate * np.sum(hidden_error, axis=0, keepdims=True) / len(x_batch)\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
        "\n",
        "# Testing the trained model\n",
        "hidden_layer_input = np.dot(x_test, weights1) + bias1\n",
        "hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "hidden_layer_input2 = np.dot(hidden_layer_output, weights2) + bias2\n",
        "hidden_layer_output2 = relu(hidden_layer_input2)\n",
        "\n",
        "output_layer_input = np.dot(hidden_layer_output2, weights3) + bias3\n",
        "predicted_output = softmax(output_layer_input)\n",
        "\n",
        "# Accuracy calculation\n",
        "accuracy = np.mean(np.argmax(predicted_output, axis=1) == np.argmax(y_test, axis=1))\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_6Yc3NlIuR0",
        "outputId": "bc72d9dd-d025-4fd1-9dfb-db0091d0e570"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.242515768939859\n",
            "Epoch 2/20, Loss: 2.1724753336504876\n",
            "Epoch 3/20, Loss: 2.0176682760295\n",
            "Epoch 4/20, Loss: 1.923807149777715\n",
            "Epoch 5/20, Loss: 1.7665662090397627\n",
            "Epoch 6/20, Loss: 1.5302282565244625\n",
            "Epoch 7/20, Loss: 1.329856216303096\n",
            "Epoch 8/20, Loss: 1.2762138145493018\n",
            "Epoch 9/20, Loss: 1.1210008666925555\n",
            "Epoch 10/20, Loss: 0.9258222263689816\n",
            "Epoch 11/20, Loss: 0.840540486772894\n",
            "Epoch 12/20, Loss: 0.8176225862630996\n",
            "Epoch 13/20, Loss: 0.6896160087524206\n",
            "Epoch 14/20, Loss: 0.6087151764770052\n",
            "Epoch 15/20, Loss: 0.5059900061939281\n",
            "Epoch 16/20, Loss: 0.7445291141681333\n",
            "Epoch 17/20, Loss: 0.5790046362484019\n",
            "Epoch 18/20, Loss: 0.6438711903988611\n",
            "Epoch 19/20, Loss: 0.4805727558506893\n",
            "Epoch 20/20, Loss: 0.6349902597717532\n",
            "Test Accuracy: 87.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL4BJlKcRGY9",
        "outputId": "b1feb446-51c7-4e20-b7b1-19a946f97e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. ResMLP Model for MNIST Dataset\n"
      ],
      "metadata": {
        "id": "EbHNlAyCSCoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the ResMLP model\n",
        "class ResMLPBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, hidden_features=None):\n",
        "        super(ResMLPBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_features or in_features),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_features or in_features, out_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ResMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_blocks=6, hidden_size=256):\n",
        "        super(ResMLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            *[ResMLPBlock(hidden_size, hidden_size) for _ in range(num_blocks)],\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = 28 * 28  # MNIST image size\n",
        "num_classes = 10  # Number of classes (digits 0-9)\n",
        "resmlp_model = ResMLP(input_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resmlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_labels in train_loader:\n",
        "        batch_inputs = batch_inputs.view(-1, input_size)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resmlp_model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Test the model\n",
        "resmlp_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_inputs, batch_labels in test_loader:\n",
        "        batch_inputs = batch_inputs.view(-1, input_size)\n",
        "        outputs = resmlp_model(batch_inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += batch_labels.size(0)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy on the test set: {100 * accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiC0EcPdRImW",
        "outputId": "e209ec8d-f0cf-4bd1-957b-ec4be579baaf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.11991137266159058\n",
            "Epoch [2/20], Loss: 0.024886859580874443\n",
            "Epoch [3/20], Loss: 0.19530010223388672\n",
            "Epoch [4/20], Loss: 0.16948872804641724\n",
            "Epoch [5/20], Loss: 0.11676311492919922\n",
            "Epoch [6/20], Loss: 0.032441675662994385\n",
            "Epoch [7/20], Loss: 0.07310124486684799\n",
            "Epoch [8/20], Loss: 0.1701682060956955\n",
            "Epoch [9/20], Loss: 0.03324282541871071\n",
            "Epoch [10/20], Loss: 0.13285356760025024\n",
            "Epoch [11/20], Loss: 0.08135014027357101\n",
            "Epoch [12/20], Loss: 0.24608370661735535\n",
            "Epoch [13/20], Loss: 0.00026121290284208953\n",
            "Epoch [14/20], Loss: 0.0901321992278099\n",
            "Epoch [15/20], Loss: 0.0021439266856759787\n",
            "Epoch [16/20], Loss: 9.47884691413492e-05\n",
            "Epoch [17/20], Loss: 0.04512358084321022\n",
            "Epoch [18/20], Loss: 0.2386184185743332\n",
            "Epoch [19/20], Loss: 0.00614657998085022\n",
            "Epoch [20/20], Loss: 0.005052010994404554\n",
            "Accuracy on the test set: 97.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NhGfLAu-jtYm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}